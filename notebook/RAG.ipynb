{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcaa2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################## Documents Loader #############################################################################################################################\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader, TextLoader, Docx2txtLoader, UnstructuredExcelLoader,\n",
    "    CSVLoader, JSONLoader, UnstructuredPowerPointLoader,\n",
    "    UnstructuredHTMLLoader, UnstructuredMarkdownLoader, UnstructuredRTFLoader,\n",
    "    UnstructuredXMLLoader, UnstructuredEmailLoader, UnstructuredEPubLoader,\n",
    "    UnstructuredImageLoader,\n",
    ")\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.ERROR)\n",
    "\n",
    "def loadDocs(dir: str):\n",
    "\n",
    "    data_path = (Path.cwd() / dir).resolve()\n",
    "    print(f\"[DEBUG] Data path: {data_path}\")\n",
    "    documents = []\n",
    "\n",
    "    if not data_path.exists():\n",
    "        print(\"[ERROR] Folder not found!\")\n",
    "        return []\n",
    "\n",
    "    # âœ… One mapping â€” extension(s) â†’ loader class\n",
    "    FILE_LOADERS = {\n",
    "        PyPDFLoader:                  [\"*.pdf\"],\n",
    "        Docx2txtLoader:               [\"*.docx\", \"*.doc\"],\n",
    "        UnstructuredExcelLoader:      [\"*.xlsx\", \"*.xls\"],\n",
    "        CSVLoader:                    [\"*.csv\"],\n",
    "        UnstructuredPowerPointLoader: [\"*.pptx\", \"*.ppt\"],\n",
    "        UnstructuredHTMLLoader:       [\"*.html\", \"*.htm\"],\n",
    "        UnstructuredMarkdownLoader:   [\"*.md\", \"*.markdown\"],\n",
    "        UnstructuredRTFLoader:        [\"*.rtf\"],\n",
    "        UnstructuredXMLLoader:        [\"*.xml\"],\n",
    "        UnstructuredEmailLoader:      [\"*.eml\", \"*.msg\"],\n",
    "        UnstructuredEPubLoader:       [\"*.epub\"],\n",
    "        UnstructuredImageLoader:      [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"],\n",
    "        TextLoader:                   [\"*.txt\", \"*.yaml\", \"*.yml\"],\n",
    "    }\n",
    "    \n",
    "\n",
    "    # âœ… Single loop handles everything\n",
    "    for loader_class, extensions in FILE_LOADERS.items():\n",
    "        files = []\n",
    "        for ext in extensions:\n",
    "            #[1.pdf , 2.pdf]\n",
    "            files.extend(data_path.glob(f'**/{ext}'))\n",
    "            \n",
    "        print(f\"[DEBUG] Found {len(files)} {extensions} files\")\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                # pypdf(1.pdf) then pypdf(1.pdf) and so on\n",
    "                loader = loader_class(str(file))\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\n Total documents loaded: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Useage\n",
    "docs = loadDocs(r\"..\\data\")\n",
    "\n",
    "if len(docs) > 0:\n",
    "    print(docs[0].page_content[:100])\n",
    "    print(docs[0].metadata)\n",
    "else:\n",
    "    print(\"No documents loaded â€” check errors above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbec230",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################## CHUNKING #############################################################################################################################\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "CHUNK_CONFIG = {\n",
    "    \"pdf\": (500, 100), \"docx\": (500, 100), \"txt\": (400, 80),\n",
    "    \"csv\": (300, 50),  \"xlsx\": (300, 50),  \"json\": (300, 50),\n",
    "    \"pptx\": (400, 80), \"html\": (500, 100), \"md\": (500, 100),\n",
    "    \"py\": (1500, 200), \"js\": (1500, 200),\n",
    "}\n",
    "\n",
    "def cleanText(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text)            # extra spaces/tabs\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)      # 3+ newlines â†’ 2\n",
    "    return text.strip()\n",
    "\n",
    "def chunkDocs(docs):\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        # clean\n",
    "        doc.page_content = cleanText(doc.page_content)\n",
    "\n",
    "        # get config based on file type\n",
    "        ext = doc.metadata.get(\"source\", \"\").split(\".\")[-1].lower()\n",
    "        size, overlap = CHUNK_CONFIG.get(ext, (500, 100))\n",
    "\n",
    "        # split\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
    "        doc_chunks = splitter.split_documents([doc])\n",
    "\n",
    "        # filter garbage + add metadata\n",
    "        for chunk in doc_chunks:\n",
    "            if len(chunk.page_content.strip()) > 50:\n",
    "                chunk.metadata[\"chunk_id\"] = len(chunks)\n",
    "                chunk.metadata[\"file_type\"] = ext\n",
    "                chunk.metadata[\"hash_id\"] = hashlib.md5(chunk.page_content.encode()).hexdigest()\n",
    "                chunks.append(chunk)\n",
    "\n",
    "    print(f\"âœ… {len(chunks)} chunks from {len(docs)} docs\")\n",
    "    return chunks\n",
    "\n",
    "chunks = chunkDocs(docs)\n",
    "if len(chunks) > 0:\n",
    "    print(chunks[0].page_content[:100])\n",
    "    print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\")\n",
    "seen_hashes = set()\n",
    "\n",
    "def EmbeddingChunkedData(chunks: List[Document]):\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"[WARNING] No chunks to embed!\")\n",
    "        return [], []\n",
    "\n",
    "    # filter duplicates using hash from metadata\n",
    "    unique_chunks = []\n",
    "    skipped = 0\n",
    "    for chunk in chunks:\n",
    "        hash_id = chunk.metadata.get(\"hash_id\")\n",
    "        if hash_id in seen_hashes:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        seen_hashes.add(hash_id)\n",
    "        unique_chunks.append(chunk)\n",
    "\n",
    "    print(f\"âœ… Unique : {len(unique_chunks)} | ðŸ” Skipped : {skipped}\")\n",
    "\n",
    "    if not unique_chunks:\n",
    "        print(\"[WARNING] All chunks were duplicates!\")\n",
    "        return [], []\n",
    "\n",
    "    # embed\n",
    "    embeddings = model.encode(\n",
    "        [chunk.page_content for chunk in unique_chunks],\n",
    "        batch_size=64,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Embedded {len(embeddings)} chunks\")\n",
    "    print(f\"ðŸ“ Shape: {len(embeddings)} x {len(embeddings[0])}\")\n",
    "    \n",
    "    return unique_chunks, embeddings\n",
    "\n",
    "\n",
    "# USEAGE\n",
    "unique_chunks, embeddings = EmbeddingChunkedData(chunks)\n",
    "\n",
    "# verify\n",
    "print(\"\\nSample chunk:\")\n",
    "print(unique_chunks[0].page_content[:300])\n",
    "print(\"\\nMetadata:\")\n",
    "print(unique_chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "STORE_DIR   = \"vector_store\"\n",
    "INDEX_PATH  = f\"{STORE_DIR}/index.faiss\"\n",
    "CHUNKS_PATH = f\"{STORE_DIR}/chunks.pkl\"\n",
    "\n",
    "# SAVE\n",
    "def saveVectorStore(chunks: List[Document], embeddings: np.ndarray):\n",
    "    os.makedirs(STORE_DIR, exist_ok=True)\n",
    "    embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "    # load existing or create new\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        index = faiss.read_index(INDEX_PATH)\n",
    "        with open(CHUNKS_PATH, \"rb\") as f:\n",
    "            existing_chunks = pickle.load(f)\n",
    "        existing_hashes = {c.metadata.get(\"hash_id\") for c in existing_chunks}\n",
    "    else:\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        existing_chunks = []\n",
    "        existing_hashes = set()\n",
    "\n",
    "    # filter duplicates\n",
    "    filtered = [(c, e) for c, e in zip(chunks, embeddings)\n",
    "                if c.metadata.get(\"hash_id\") not in existing_hashes]\n",
    "\n",
    "    if not filtered:\n",
    "        print(\"âœ… No new chunks â€” everything already stored!\")\n",
    "        return\n",
    "\n",
    "    new_chunks, new_embeddings = zip(*filtered)\n",
    "\n",
    "    index.add(np.array(new_embeddings).astype(\"float32\"))\n",
    "    all_chunks = existing_chunks + list(new_chunks)\n",
    "\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    with open(CHUNKS_PATH, \"wb\") as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "\n",
    "    print(f\"âœ… Added: {len(new_chunks)} | Total: {index.ntotal} vectors\")\n",
    "\n",
    "# Usage\n",
    "saveVectorStore(unique_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "def loadVectorStore():\n",
    "    if not os.path.exists(INDEX_PATH):\n",
    "        print(\"[ERROR] No vector store found!\")\n",
    "        return None, []\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(CHUNKS_PATH, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    print(f\"âœ… Loaded {index.ntotal} vectors\")\n",
    "    return index, chunks\n",
    "\n",
    "\n",
    "# SEARCH\n",
    "def search(query: str, top_k: int = 3):\n",
    "    index, chunks = loadVectorStore()\n",
    "    if index is None:\n",
    "        return []\n",
    "\n",
    "    query_emb = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    scores, indices = index.search(query_emb, top_k)\n",
    "\n",
    "    return [\n",
    "    {\n",
    "        \"content\": chunks[i].page_content,       # actual text\n",
    "        \"score\":   round(float(s), 4),            # similarity score\n",
    "        \"source\":  chunks[i].metadata.get(\"source\") # which file\n",
    "    }\n",
    "    for s, i in zip(scores[0], indices[0])  # pair each score with its index\n",
    "    if i != -1                               # -1 means no result found â†’ skip\n",
    "    ]\n",
    "    \n",
    "results = search(\"can you tell me how much root gap is allowed as per the saes?\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} | Score: {r['score']} ---\")\n",
    "    print(r[\"content\"][:300])\n",
    "    print(\"Source:\", r[\"source\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22226734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] API_KEY: set\n",
      "[DEBUG] API_URL: https://openrouter.ai/api/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "API_URL = os.getenv(\"API_URL\")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a technical engineering assistant.\n",
    "Answer ONLY from the context provided.\n",
    "If answer is not in context, say \"Not found in the documents.\"\n",
    "Do NOT use your own knowledge.\"\"\"\n",
    "\n",
    "def GenerateAnswer(query: str, results: List[dict]) -> str:\n",
    "\n",
    "    # âœ… lower threshold â€” 0.35 is fine for technical docs\n",
    "    top_results = [r for r in results if r[\"score\"] > 0.35]\n",
    "\n",
    "    if not top_results:\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    # âœ… clean context format â€” just text + source, no dict noise\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {r['source'].split(chr(92))[-1]}\\n{r['content']}\"\n",
    "        for r in top_results\n",
    "    ])\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer strictly from the context:\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"openai/gpt-3.5-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"HTTP-Referer\": \"http://localhost\",\n",
    "        \"X-Title\": \"RAG App\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload, headers=headers)\n",
    "        result = response.json()\n",
    "\n",
    "        if \"error\" in result:\n",
    "            print(f\"[ERROR] API: {result['error']}\")\n",
    "            return None\n",
    "\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "# USEAGE\n",
    "question = \"can you tell me about seas-108?\"\n",
    "results = search(question)\n",
    "answer  = GenerateAnswer(question, results)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-FunctionalBase-latest",
   "language": "python",
   "name": "rag-functionalbase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
